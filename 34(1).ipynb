{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ffd42d8-7ca3-4b94-9296-adbba9fdc0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c661f5d0-f744-40d7-a18b-bccbf858048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 # 28x28\n",
    "hidden_size = 500 \n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b8d72ca-2340-463b-8b55-feec2435b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a61bc0c-ce71-4f59-8aac-396423e12dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "        nn.Conv2d(1,64,kernel_size=3, padding = 1),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.Conv2d(64,128,kernel_size=3, padding = 1),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.Conv2d(128,64,kernel_size=3, padding = 1),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2), stride = 2))\n",
    "        self.nn = nn.Sequential(nn.Linear(64,20,bias = True),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(20,10,bias = True))\n",
    "    def forward(self, x):\n",
    "        output = self.net(x)\n",
    "        return self.nn(output.view(100,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdd8a3ff-0289-49ba-9307-577d291dbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3554974-024a-4d62-a47f-589f191c4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "102a4516-964b-4a10-95e8-8843d0718628",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1886fee0-35be-46ca-b206-079f6769e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86d5184d-0b83-43de-bf30-0e08a25d4733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def trainModel(model, num_epochs,train_loader,optimizer):\n",
    "    n_total_steps = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # origin shape: [100, 1, 28, 28]\n",
    "            # resized: [100, 784]\n",
    "            # images = images.reshape(-1, 28*28).to(device)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            if (i+1) % 100 == 0:\n",
    "                print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fb94337-c24d-4c83-9976-79f65f756382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/600], Loss: 0.4494\n",
      "Epoch [1/1], Step [200/600], Loss: 0.2115\n",
      "Epoch [1/1], Step [300/600], Loss: 0.1300\n",
      "Epoch [1/1], Step [400/600], Loss: 0.1775\n",
      "Epoch [1/1], Step [500/600], Loss: 0.1908\n",
      "Epoch [1/1], Step [600/600], Loss: 0.1489\n"
     ]
    }
   ],
   "source": [
    "trainModel(model,1, train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90d6fe82-9793-41c0-ba91-ef69329adc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "        nn.Conv2d(1,32,kernel_size=3, padding = 1),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.Conv2d(32,64,kernel_size=3, padding = 1),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.Conv2d(64,32,kernel_size=3, padding = 1),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2), stride = 2))\n",
    "        self.nn = nn.Sequential(nn.Linear(32,20,bias = True),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(20,10,bias = True))\n",
    "    def forward(self, x):\n",
    "        output = self.net(x)\n",
    "        return self.nn(output.view(100,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34b6642c-21b8-4016-9f08-150ec06d2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "        nn.Conv2d(1,16,kernel_size=3, padding = 1),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.Conv2d(16,32,kernel_size=3, padding = 1),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.Conv2d(32,16,kernel_size=3, padding = 1),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2), stride = 2))\n",
    "        self.nn = nn.Sequential(nn.Linear(16,20,bias = True),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(20,10,bias = True))\n",
    "    def forward(self, x):\n",
    "        output = self.net(x)\n",
    "        return self.nn(output.view(100,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16cca14e-e671-4446-8969-b2bdf378284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "        nn.Conv2d(1,64,kernel_size=3, padding = 2),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.Conv2d(64,128,kernel_size=3, padding = 2),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.Conv2d(128,256,kernel_size=3, padding = 2),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.Conv2d(256,128,kernel_size=3, padding = 2),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.Conv2d(128,64,kernel_size=3, padding = 2),\n",
    "        nn.MaxPool2d((2,2), stride = 2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2), stride = 2))\n",
    "        self.nn = nn.Sequential(nn.Linear(64,20,bias = True),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(20,10,bias = True))\n",
    "    def forward(self, x):\n",
    "        output = self.net(x)\n",
    "        return self.nn(output.view(100,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29089036-3991-49cb-a45a-ce20108f6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay as cdm\n",
    "def test(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()  \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66f23fed-09f2-4327-8580-ee3f30789dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = CNN().to(device)\n",
    "m2 = CNN2().to(device)\n",
    "m3 = CNN3().to(device)\n",
    "m4 = CNN4().to(device)\n",
    "\n",
    "o1 = torch.optim.Adam(m1.parameters(), lr=learning_rate)\n",
    "o2 = torch.optim.Adam(m2.parameters(), lr=learning_rate)\n",
    "o3 = torch.optim.Adam(m3.parameters(), lr=learning_rate)\n",
    "o4 = torch.optim.Adam(m4.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a795e4ce-f095-4c3b-bacc-7693d619b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.4517\n",
      "Epoch [1/2], Step [200/600], Loss: 0.2370\n",
      "Epoch [1/2], Step [300/600], Loss: 0.3981\n",
      "Epoch [1/2], Step [400/600], Loss: 0.1176\n",
      "Epoch [1/2], Step [500/600], Loss: 0.2224\n",
      "Epoch [1/2], Step [600/600], Loss: 0.2107\n",
      "Epoch [2/2], Step [100/600], Loss: 0.2207\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0982\n",
      "Epoch [2/2], Step [300/600], Loss: 0.1471\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0782\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0970\n",
      "Epoch [2/2], Step [600/600], Loss: 0.1547\n",
      "Epoch [1/2], Step [100/600], Loss: 0.9612\n",
      "Epoch [1/2], Step [200/600], Loss: 0.7090\n",
      "Epoch [1/2], Step [300/600], Loss: 0.6129\n",
      "Epoch [1/2], Step [400/600], Loss: 0.4518\n",
      "Epoch [1/2], Step [500/600], Loss: 0.2522\n",
      "Epoch [1/2], Step [600/600], Loss: 0.2983\n",
      "Epoch [2/2], Step [100/600], Loss: 0.1883\n",
      "Epoch [2/2], Step [200/600], Loss: 0.1612\n",
      "Epoch [2/2], Step [300/600], Loss: 0.1892\n",
      "Epoch [2/2], Step [400/600], Loss: 0.2734\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0994\n",
      "Epoch [2/2], Step [600/600], Loss: 0.2390\n",
      "Epoch [1/2], Step [100/600], Loss: 0.1845\n",
      "Epoch [1/2], Step [200/600], Loss: 0.1772\n",
      "Epoch [1/2], Step [300/600], Loss: 0.1948\n",
      "Epoch [1/2], Step [400/600], Loss: 0.2022\n",
      "Epoch [1/2], Step [500/600], Loss: 0.1139\n",
      "Epoch [1/2], Step [600/600], Loss: 0.0198\n",
      "Epoch [2/2], Step [100/600], Loss: 0.0247\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0457\n",
      "Epoch [2/2], Step [300/600], Loss: 0.0643\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0529\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0191\n",
      "Epoch [2/2], Step [600/600], Loss: 0.0423\n"
     ]
    }
   ],
   "source": [
    "#trainModel(m1,2, train_loader, o1)\n",
    "trainModel(m2,2, train_loader, o2)\n",
    "trainModel(m3,2, train_loader, o3)\n",
    "trainModel(m4,2, train_loader, o4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eddad5cf-c5e4-4ac5-b4a4-29bcce8793c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2a = test(m2,test_loader)\n",
    "m3a = test(m3,test_loader)\n",
    "m4a = test(m4,test_loader)\n",
    "m1a = test(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "022ed11a-5a1d-44a2-9c2c-d7019e61c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d8947a9-d9bd-4df7-9dc5-43b4cfd9898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2p = count_parameters(m2)\n",
    "m3p = count_parameters(m3)\n",
    "m4p = count_parameters(m4)\n",
    "m1p = count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "76236b10-0f16-46a4-82a7-9f8d53b593c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = [m1p,m1p-m2p,m1p-m3p,m4p-m1p]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9013113e-a14c-4c1e-bb48-c7288b710891",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34c0c89f-35d6-423f-8de5-f698ce10c160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100.        ,  74.53237026,  93.34170016, 394.00259015])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x / m1p * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "181458e4-51ff-4761-93ed-47c6811b09ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f6b480abc10>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = np.array([m1a, m2a, m3a, m4a])\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80b7e367-95f3-4413-b9a8-83c4f1c03821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9575, 0.9688, 0.9468, 0.9888])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9c8c6be-d584-488d-bf13-5468f54b1f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([149798, 111648, 139824, 590208])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da39018-74a0-43f7-858e-a0e61ecfdde6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ada61-8310-4384-8cfd-2e9414d4b2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e20ee-fe1c-4e51-853a-8fcddea65754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q1\n",
    "\n",
    "Implement convolution operation for a sample image of shape (H=6, W=6, C=1) with a random kernel of size (3,3) using torch.nn.functional.conv2d.\n",
    "\n",
    "What is the dimension of the output image? Apply, various values for parameter stride=1 and note the change in the dimension of the output image. Arrive at an equation for the output image size with respect to the kernel size and stride and verify your answer with code. Now, repeat the exercise by changing padding parameter. Obtain a formula using kernel, stride, and padding to get the output image size. What is the total number of parameters in your network? Verify with code.\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "image = torch.rand(6,6)\n",
    "print(\"image=\", image)\n",
    "#Add a new dimension along 0th dimension\n",
    "#i.e. (6,6) becomes (1,6,6). This is because\n",
    "#pytorch expects the input to conv2D as 4d tensor\n",
    "image = image.unsqueeze(dim=0)\n",
    "print(\"image.shape=\", image.shape)\n",
    "image = image.unsqueeze(dim=0)\n",
    "print(\"image.shape=\", image.shape)\n",
    "print(\"image=\", image)\n",
    "kernel = torch.ones(3,3)\n",
    "#kernel = torch.rand(3,3)\n",
    "print(\"kernel=\", kernel)\n",
    "kernel = kernel.unsqueeze(dim=0)\n",
    "kernel = kernel.unsqueeze(dim=0)\n",
    "\n",
    "def out_dim(in_shape,stride,padding,kernel_shape):\n",
    "    out_shape = [0 for i in range(4)]\n",
    "    for dim in range(len(in_shape)):\n",
    "        out_shape[dim] = (in_shape[dim] + 2*padding - kernel_shape[dim])//stride + 1\n",
    "    return out_shape\n",
    "    \n",
    "#Stride 1 Padding 0\n",
    "outimage = F.conv2d(image, kernel, stride=1, padding=0)\n",
    "print(\"outimage=\", outimage)\n",
    "print(\"Dimension of output image S-1 P-0: \",outimage.shape)\n",
    "print(\"Manually dim of output S-1 P-0: \",out_dim(image.shape,1,0,kernel.shape))\n",
    "\n",
    "#Stride 1 Padding 1\n",
    "outimage = F.conv2d(image, kernel, stride=1, padding=1)\n",
    "print(\"Dimension of output image S-1 P-1:\",outimage.shape)\n",
    "print(\"Manually dim of output S-1 P-1: \",out_dim(image.shape,1,1,kernel.shape))\n",
    "\n",
    "#Stride 1 Padding 2\n",
    "outimage = F.conv2d(image, kernel, stride=1, padding=2)\n",
    "print(\"Dimension of output image S-1 P-2:\",outimage.shape)\n",
    "print(\"Manually dim of output S-1 P-2: \",out_dim(image.shape,1,2,kernel.shape))\n",
    "\n",
    "#Stride 2 Padding 1\n",
    "outimage = F.conv2d(image, kernel, stride=2, padding=1)\n",
    "print(\"Dimension of output image S-2 P-1: \",outimage.shape)\n",
    "print(\"Manually dim of output S-2 P-1: \",out_dim(image.shape,2,1,kernel.shape))\n",
    "\n",
    "#Stride 3 Padding 1\n",
    "outimage = F.conv2d(image, kernel, stride=3, padding=1)\n",
    "print(\"Dimension of output image S-2 P-1:\",outimage.shape)\n",
    "print(\"Manually dim of output S-3 P-1: \",out_dim(image.shape,3,1,kernel.shape))\n",
    "\n",
    "print(\"Number of Learnable Parameters = 9\")\n",
    "\n",
    "image= tensor([[0.1186, 0.7337, 0.9362, 0.6941, 0.9227, 0.1054],\n",
    "        [0.0437, 0.4127, 0.3661, 0.6174, 0.4510, 0.1018],\n",
    "        [0.0351, 0.9308, 0.9723, 0.7033, 0.8682, 0.1331],\n",
    "        [0.3270, 0.2197, 0.3275, 0.5457, 0.7827, 0.8512],\n",
    "        [0.8301, 0.7932, 0.5169, 0.0567, 0.7926, 0.9195],\n",
    "        [0.8814, 0.2048, 0.5625, 0.8749, 0.1683, 0.0961]])\n",
    "image.shape= torch.Size([1, 6, 6])\n",
    "image.shape= torch.Size([1, 1, 6, 6])\n",
    "image= tensor([[[[0.1186, 0.7337, 0.9362, 0.6941, 0.9227, 0.1054],\n",
    "          [0.0437, 0.4127, 0.3661, 0.6174, 0.4510, 0.1018],\n",
    "          [0.0351, 0.9308, 0.9723, 0.7033, 0.8682, 0.1331],\n",
    "          [0.3270, 0.2197, 0.3275, 0.5457, 0.7827, 0.8512],\n",
    "          [0.8301, 0.7932, 0.5169, 0.0567, 0.7926, 0.9195],\n",
    "          [0.8814, 0.2048, 0.5625, 0.8749, 0.1683, 0.0961]]]])\n",
    "kernel= tensor([[1., 1., 1.],\n",
    "        [1., 1., 1.],\n",
    "        [1., 1., 1.]])\n",
    "outimage= tensor([[[[4.5493, 6.3667, 6.5314, 4.5971],\n",
    "          [3.6350, 5.0957, 5.6344, 5.0546],\n",
    "          [4.9527, 5.0661, 5.5659, 5.6531],\n",
    "          [4.6631, 4.1019, 4.6277, 5.0876]]]])\n",
    "Dimension of output image S-1 P-0:  torch.Size([1, 1, 4, 4])\n",
    "Manually dim of output S-1 P-0:  [1, 1, 4, 4]\n",
    "Dimension of output image S-1 P-1: torch.Size([1, 1, 6, 6])\n",
    "Manually dim of output S-1 P-1:  [3, 3, 6, 6]\n",
    "Dimension of output image S-1 P-2: torch.Size([1, 1, 8, 8])\n",
    "Manually dim of output S-1 P-2:  [5, 5, 8, 8]\n",
    "Dimension of output image S-2 P-1:  torch.Size([1, 1, 3, 3])\n",
    "Manually dim of output S-2 P-1:  [2, 2, 3, 3]\n",
    "Dimension of output image S-2 P-1: torch.Size([1, 1, 2, 2])\n",
    "Manually dim of output S-3 P-1:  [1, 1, 2, 2]\n",
    "Number of Learnable Parameters = 9\n",
    "\n",
    "Q2\n",
    "\n",
    "Apply torch.nn.Conv2d to the input image of Qn 1 with out-channel=3 and observe the output. Implement the equivalent of torch.nn.Conv2d using the torch.nn.functional.conv2D to get the same output. You may ignore bias.\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "image= torch.tensor([[[[0.2557, 0.9236, 0.4913, 0.3200, 0.4958, 0.2214],\n",
    "          [0.7554, 0.6501, 0.0107, 0.8675, 0.5163, 0.6102],\n",
    "          [0.8228, 0.1919, 0.8724, 0.8043, 0.3882, 0.9689],\n",
    "          [0.4894, 0.5116, 0.5624, 0.6949, 0.6289, 0.9802],\n",
    "          [0.3913, 0.2773, 0.1427, 0.3717, 0.4154, 0.3669],\n",
    "          [0.8327, 0.8157, 0.7192, 0.9387, 0.4569, 0.6776]]]])\n",
    "\n",
    "conv = nn.Conv2d(in_channels=1,out_channels=3,kernel_size=3,stride=1,padding=0,bias=False)\n",
    "print(\"Kernel parameters for 3 channels: \")\n",
    "kernel = conv.weight\n",
    "print(conv.weight)\n",
    "print(\"Output image using torch.nn.Conv2d: \")\n",
    "out_image = print(conv(image))\n",
    "\n",
    "import torch.nn.functional as F\n",
    "out_image = F.conv2d(image,kernel,stride=1,padding=0)\n",
    "print(\"Output image using torch.nn.functional.conv2d: \")\n",
    "print(out_image)\n",
    "\n",
    "Kernel parameters for 3 channels: \n",
    "Parameter containing:\n",
    "tensor([[[[-0.0442, -0.0062,  0.1765],\n",
    "          [ 0.0043, -0.3088,  0.3210],\n",
    "          [ 0.1142, -0.2003, -0.3295]]],\n",
    "\n",
    "\n",
    "        [[[ 0.2522, -0.1952,  0.2530],\n",
    "          [ 0.0576, -0.2026, -0.1792],\n",
    "          [-0.0216,  0.1752,  0.1663]]],\n",
    "\n",
    "\n",
    "        [[[ 0.1334,  0.2059, -0.1624],\n",
    "          [ 0.0182, -0.1681,  0.0662],\n",
    "          [-0.0531, -0.0890, -0.0859]]]], requires_grad=True)\n",
    "Output image using torch.nn.Conv2d: \n",
    "tensor([[[[-0.3563, -0.1274, -0.2278, -0.2432],\n",
    "          [-0.0432, -0.1694, -0.3169, -0.1089],\n",
    "          [ 0.0831,  0.0602, -0.1804,  0.0947],\n",
    "          [-0.2690, -0.1873, -0.1555, -0.0782]],\n",
    "\n",
    "         [[ 0.0793,  0.3803,  0.1059,  0.0877],\n",
    "          [ 0.0911,  0.2746, -0.0041,  0.3246],\n",
    "          [ 0.2784, -0.0466,  0.0713,  0.2349],\n",
    "          [ 0.3512,  0.3800,  0.2487,  0.3445]],\n",
    "\n",
    "         [[-0.0861,  0.0829, -0.2119, -0.0823],\n",
    "          [ 0.1535, -0.2789, -0.1432, -0.0407],\n",
    "          [-0.0901, -0.0240,  0.0776, -0.0866],\n",
    "          [-0.1294, -0.1112, -0.0775, -0.1246]]]],\n",
    "       grad_fn=<ConvolutionBackward0>)\n",
    "Output image using torch.nn.functional.conv2d: \n",
    "tensor([[[[-0.3563, -0.1274, -0.2278, -0.2432],\n",
    "          [-0.0432, -0.1694, -0.3169, -0.1089],\n",
    "          [ 0.0831,  0.0602, -0.1804,  0.0947],\n",
    "          [-0.2690, -0.1873, -0.1555, -0.0782]],\n",
    "\n",
    "         [[ 0.0793,  0.3803,  0.1059,  0.0877],\n",
    "          [ 0.0911,  0.2746, -0.0041,  0.3246],\n",
    "          [ 0.2784, -0.0466,  0.0713,  0.2349],\n",
    "          [ 0.3512,  0.3800,  0.2487,  0.3445]],\n",
    "\n",
    "         [[-0.0861,  0.0829, -0.2119, -0.0823],\n",
    "          [ 0.1535, -0.2789, -0.1432, -0.0407],\n",
    "          [-0.0901, -0.0240,  0.0776, -0.0866],\n",
    "          [-0.1294, -0.1112, -0.0775, -0.1246]]]],\n",
    "       grad_fn=<ConvolutionBackward0>)\n",
    "\n",
    "Q3\n",
    "\n",
    "Implement CNN for classifying digits in MNIST dataset using PyTorch. Display the classification accuracy in the form of a Confusion matrix. Verify the number of learnable parameters in the model.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,64,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(64,128,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(128,64,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(64,20,bias=True),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(20,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root=\"./data\",download = True,train=True,transform=ToTensor())\n",
    "train_loader = DataLoader(mnist_trainset,batch_size=50,shuffle=True)\n",
    "mnist_testset = datasets.MNIST(root=\"./data\",download = True,train=False,transform=ToTensor())\n",
    "test_loader = DataLoader(mnist_testset,batch_size=50,shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNClassifier().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "\n",
    "total_params = 0\n",
    "for name,param in model.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "correct,total = 0,0\n",
    "for i,vdata in enumerate(test_loader):\n",
    "    tinputs,tlabels = vdata[0].to(device), vdata[1].to(device)\n",
    "    toutputs = model(tinputs)\n",
    "\n",
    "    _,predicted = torch.max(toutputs,1)\n",
    "    total += tlabels.size(0)\n",
    "    correct += (predicted==tlabels).sum()\n",
    "        \n",
    "print(f\"Correct = {correct}, Total = {total}\")\n",
    "\n",
    "[1,   100] loss: 2.313\n",
    "[1,   200] loss: 2.301\n",
    "[1,   300] loss: 2.291\n",
    "[1,   400] loss: 2.278\n",
    "[1,   500] loss: 2.259\n",
    "[1,   600] loss: 2.221\n",
    "[1,   700] loss: 2.145\n",
    "[1,   800] loss: 1.973\n",
    "[1,   900] loss: 1.635\n",
    "[1,  1000] loss: 1.279\n",
    "[1,  1100] loss: 1.000\n",
    "[1,  1200] loss: 0.809\n",
    "[2,   100] loss: 0.721\n",
    "[2,   200] loss: 0.619\n",
    "[2,   300] loss: 0.535\n",
    "[2,   400] loss: 0.496\n",
    "[2,   500] loss: 0.429\n",
    "[2,   600] loss: 0.412\n",
    "[2,   700] loss: 0.364\n",
    "[2,   800] loss: 0.332\n",
    "[2,   900] loss: 0.314\n",
    "[2,  1000] loss: 0.303\n",
    "[2,  1100] loss: 0.302\n",
    "[2,  1200] loss: 0.296\n",
    "[3,   100] loss: 0.247\n",
    "[3,   200] loss: 0.259\n",
    "[3,   300] loss: 0.252\n",
    "[3,   400] loss: 0.242\n",
    "[3,   500] loss: 0.234\n",
    "[3,   600] loss: 0.220\n",
    "[3,   700] loss: 0.226\n",
    "[3,   800] loss: 0.209\n",
    "[3,   900] loss: 0.226\n",
    "[3,  1000] loss: 0.208\n",
    "[3,  1100] loss: 0.206\n",
    "[3,  1200] loss: 0.188\n",
    "[4,   100] loss: 0.187\n",
    "[4,   200] loss: 0.184\n",
    "[4,   300] loss: 0.179\n",
    "[4,   400] loss: 0.178\n",
    "[4,   500] loss: 0.179\n",
    "[4,   600] loss: 0.177\n",
    "[4,   700] loss: 0.171\n",
    "[4,   800] loss: 0.162\n",
    "[4,   900] loss: 0.168\n",
    "[4,  1000] loss: 0.153\n",
    "[4,  1100] loss: 0.155\n",
    "[4,  1200] loss: 0.154\n",
    "[5,   100] loss: 0.154\n",
    "[5,   200] loss: 0.142\n",
    "[5,   300] loss: 0.161\n",
    "[5,   400] loss: 0.137\n",
    "[5,   500] loss: 0.153\n",
    "[5,   600] loss: 0.127\n",
    "[5,   700] loss: 0.146\n",
    "[5,   800] loss: 0.134\n",
    "[5,   900] loss: 0.126\n",
    "[5,  1000] loss: 0.119\n",
    "[5,  1100] loss: 0.125\n",
    "[5,  1200] loss: 0.119\n",
    "[6,   100] loss: 0.138\n",
    "[6,   200] loss: 0.114\n",
    "[6,   300] loss: 0.115\n",
    "[6,   400] loss: 0.123\n",
    "[6,   500] loss: 0.118\n",
    "[6,   600] loss: 0.125\n",
    "[6,   700] loss: 0.113\n",
    "[6,   800] loss: 0.114\n",
    "[6,   900] loss: 0.129\n",
    "[6,  1000] loss: 0.111\n",
    "[6,  1100] loss: 0.102\n",
    "[6,  1200] loss: 0.105\n",
    "Finished Training. Final loss = 0.18399189412593842, Total params = 149798\n",
    "Correct = 9711, Total = 10000\n",
    "\n",
    "Q4\n",
    "\n",
    "Modify CNN of Qn. 3 to reduce the number of parameters in the network. Draw a plot of percentage drop in parameters vs accuracy.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class CNNClassifier1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,16,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(16,32,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(32,16,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(16,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root=\"./data\",download = True,train=True,transform=ToTensor())\n",
    "train_loader = DataLoader(mnist_trainset,batch_size=50,shuffle=True)\n",
    "mnist_testset = datasets.MNIST(root=\"./data\",download = True,train=False,transform=ToTensor())\n",
    "test_loader = DataLoader(mnist_testset,batch_size=50,shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = CNNClassifier1().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model1.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "\n",
    "total_params = 0\n",
    "for name,param in model1.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model1(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "correct,total = 0,0\n",
    "for i,vdata in enumerate(test_loader):\n",
    "    tinputs,tlabels = vdata[0].to(device), vdata[1].to(device)\n",
    "    toutputs = model1(tinputs)\n",
    "\n",
    "    _,predicted = torch.max(toutputs,1)\n",
    "    total += tlabels.size(0)\n",
    "    correct += (predicted==tlabels).sum()\n",
    "        \n",
    "print(f\"Correct = {correct}, Total = {total}\")\n",
    "\n",
    "[1,   100] loss: 2.313\n",
    "[1,   200] loss: 2.300\n",
    "[1,   300] loss: 2.287\n",
    "[1,   400] loss: 2.270\n",
    "[1,   500] loss: 2.249\n",
    "[1,   600] loss: 2.204\n",
    "[1,   700] loss: 2.114\n",
    "[1,   800] loss: 1.891\n",
    "[1,   900] loss: 1.515\n",
    "[1,  1000] loss: 1.140\n",
    "[1,  1100] loss: 0.913\n",
    "[1,  1200] loss: 0.829\n",
    "[2,   100] loss: 0.754\n",
    "[2,   200] loss: 0.701\n",
    "[2,   300] loss: 0.642\n",
    "[2,   400] loss: 0.581\n",
    "[2,   500] loss: 0.530\n",
    "[2,   600] loss: 0.526\n",
    "[2,   700] loss: 0.484\n",
    "[2,   800] loss: 0.491\n",
    "[2,   900] loss: 0.457\n",
    "[2,  1000] loss: 0.423\n",
    "[2,  1100] loss: 0.415\n",
    "[2,  1200] loss: 0.398\n",
    "[3,   100] loss: 0.371\n",
    "[3,   200] loss: 0.332\n",
    "[3,   300] loss: 0.363\n",
    "[3,   400] loss: 0.339\n",
    "[3,   500] loss: 0.348\n",
    "[3,   600] loss: 0.330\n",
    "[3,   700] loss: 0.317\n",
    "[3,   800] loss: 0.321\n",
    "[3,   900] loss: 0.287\n",
    "[3,  1000] loss: 0.287\n",
    "[3,  1100] loss: 0.287\n",
    "[3,  1200] loss: 0.282\n",
    "[4,   100] loss: 0.269\n",
    "[4,   200] loss: 0.287\n",
    "[4,   300] loss: 0.243\n",
    "[4,   400] loss: 0.258\n",
    "[4,   500] loss: 0.268\n",
    "[4,   600] loss: 0.240\n",
    "[4,   700] loss: 0.235\n",
    "[4,   800] loss: 0.235\n",
    "[4,   900] loss: 0.246\n",
    "[4,  1000] loss: 0.209\n",
    "[4,  1100] loss: 0.229\n",
    "[4,  1200] loss: 0.241\n",
    "[5,   100] loss: 0.205\n",
    "[5,   200] loss: 0.209\n",
    "[5,   300] loss: 0.243\n",
    "[5,   400] loss: 0.207\n",
    "[5,   500] loss: 0.211\n",
    "[5,   600] loss: 0.194\n",
    "[5,   700] loss: 0.197\n",
    "[5,   800] loss: 0.195\n",
    "[5,   900] loss: 0.197\n",
    "[5,  1000] loss: 0.177\n",
    "[5,  1100] loss: 0.200\n",
    "[5,  1200] loss: 0.197\n",
    "[6,   100] loss: 0.197\n",
    "[6,   200] loss: 0.196\n",
    "[6,   300] loss: 0.182\n",
    "[6,   400] loss: 0.173\n",
    "[6,   500] loss: 0.194\n",
    "[6,   600] loss: 0.187\n",
    "[6,   700] loss: 0.165\n",
    "[6,   800] loss: 0.158\n",
    "[6,   900] loss: 0.161\n",
    "[6,  1000] loss: 0.153\n",
    "[6,  1100] loss: 0.165\n",
    "[6,  1200] loss: 0.165\n",
    "Finished Training. Final loss = 0.20178936421871185, Total params = 9594\n",
    "Correct = 9481, Total = 10000\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root=\"./data\",download = True,train=True,transform=ToTensor())\n",
    "train_loader = DataLoader(mnist_trainset,batch_size=50,shuffle=True)\n",
    "mnist_testset = datasets.MNIST(root=\"./data\",download = True,train=False,transform=ToTensor())\n",
    "test_loader = DataLoader(mnist_testset,batch_size=50,shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "class CNNClassifier1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,128,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(128,256,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(256,128,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(128,64,bias=True),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(64,20,bias=True),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(20,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "\n",
    "class CNNClassifier2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,32,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(32,64,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(64,32,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(32,20,bias=True),\n",
    "                                                 nn.ReLU(),\n",
    "                                                 nn.Linear(20,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "    \n",
    "class CNNClassifier3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(1,16,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(16,32,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                 nn.Conv2d(32,16,3),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.MaxPool2d((2,2),stride=2),\n",
    "                                )\n",
    "        self.classification_head = nn.Sequential(nn.Linear(16,10,bias=True),)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        features = self.net(x)\n",
    "        return self.classification_head(features.view(batch_size,-1))\n",
    "\n",
    "model1 = CNNClassifier1().to(device)\n",
    "model2 = CNNClassifier2().to(device)\n",
    "model3 = CNNClassifier3().to(device)\n",
    "optimizer = optim.SGD(model1.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "loss = None\n",
    "total_params = 0\n",
    "\n",
    "for name,param in model1.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model1(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "loss = None\n",
    "total_params = 0\n",
    "\n",
    "for name,param in model2.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model2(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "\n",
    "loss = None\n",
    "optimizer = optim.SGD(model3.parameters(), lr=0.01)\n",
    "batch_size=50\n",
    "\n",
    "total_params = 0\n",
    "for name,param in model3.named_parameters():\n",
    "    params = param.numel()\n",
    "    total_params += params\n",
    "\n",
    "for epoch in range(6):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model3(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  \n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(f\"Finished Training. Final loss = {loss.item()}, Total params = {total_params}\")\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
